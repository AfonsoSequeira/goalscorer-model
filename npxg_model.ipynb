{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47d974f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import binom, poisson, nbinom, expon, gamma\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from catboost import CatBoostRegressor\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'C:/MyDevelopment/Goalscorers')\n",
    "from helper_functions import *\n",
    "import data_cleaning as dc\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66a0f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_exps = pd.read_csv(\"C:/MyDevelopment/Goalscorers/goal_expectancies/fbref_matched_expectancies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cb1e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_processed_data(seasons_to_load, leagues_to_load, data_exp):\n",
    "    data = dc.load_data(seasons_to_load, leagues_to_load)\n",
    "    data = dc.add_datetime(data)\n",
    "    data = dc.add_opposite_team(data)\n",
    "    data = dc.map_position(data)\n",
    "    data = dc.add_npg(data)\n",
    "    data = dc.add_year_week(data)\n",
    "    data = dc.add_goal_expectancies(data,data_exp)\n",
    "    data = dc.add_supremacy(data)\n",
    "    data = dc.add_npxg_per_minute(data)\n",
    "    data = dc.add_team_scored_and_conceded_npxg(data)\n",
    "    data = dc.add_solo_striker_position(data)\n",
    "    data = dc.add_main_opposing_gk(data)\n",
    "    data = dc.remove_gk(data)\n",
    "    data = dc.drop_NAs(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "preprocessed_data = get_pre_processed_data(None, None, goal_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "469d325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##filter data\n",
    "data = preprocessed_data.copy(deep=True)\n",
    "data = data[((data.season == '2017-2018') | (data.season == '2018-2019') | (data.season == '2019-2020') | (data.season == '2020-2021'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95e7fce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "league_name     season   \n",
       "Bundesliga      2017-2018     7892\n",
       "                2018-2019     7875\n",
       "                2019-2020     8132\n",
       "                2020-2021     8652\n",
       "Championship    2018-2019    14088\n",
       "                2019-2020    14356\n",
       "                2020-2021    14837\n",
       "Eredivisie      2018-2019     7774\n",
       "                2019-2020     5913\n",
       "                2020-2021     8417\n",
       "La Liga         2017-2018     9785\n",
       "                2018-2019     9829\n",
       "                2019-2020    10181\n",
       "                2020-2021    10841\n",
       "Ligue 1         2017-2018     9749\n",
       "                2018-2019     9772\n",
       "                2019-2020     7173\n",
       "                2020-2021    10725\n",
       "Premier League  2017-2018     9682\n",
       "                2018-2019     8747\n",
       "                2019-2020     8873\n",
       "                2020-2021     8663\n",
       "Primeira Liga   2018-2019     7870\n",
       "                2019-2020     8097\n",
       "                2020-2021     8718\n",
       "Serie A         2017-2018     9817\n",
       "                2018-2019     9783\n",
       "                2019-2020    10160\n",
       "                2020-2021    10839\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"league_name\",\"season\"]].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e1cf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature engineering\n",
    "\n",
    "#add average npxg per minute features \n",
    "data = fe.add_player_avg_feature(data, \"npxg_per_min\", 5)\n",
    "data = fe.add_player_avg_feature(data, \"npxg_per_min\", 10)\n",
    "\n",
    "#add average npxg per minute / average team conceded npxg features, for 2,6,10 previous appearances\n",
    "data, team_data = fe.add_team_avg_feature(data, \"team_conceded_npxg\", 5)\n",
    "data[\"npxg_per_min_over_squad_opp_avg\"] = data.npxg_per_min - data.avg_team_conceded_npxg_l5 \n",
    "data = fe.add_player_avg_feature(data, \"npxg_per_min_over_squad_opp_avg\", 5)\n",
    "data = fe.add_player_avg_feature(data, \"npxg_per_min_over_squad_opp_avg\", 10)\n",
    "\n",
    "#add average touches in att_3rd and touches in att_pen_area in last 5 games\n",
    "data[\"touches_att_3rd_per_min\"] = data.touches_att_3rd/data.minutes\n",
    "data[\"touches_att_pen_area_per_min\"] = data.touches_att_pen_area/data.minutes\n",
    "\n",
    "data = fe.add_player_avg_feature(data, \"touches_att_3rd_per_min\", 5)\n",
    "data = fe.add_player_avg_feature(data, \"touches_att_pen_area_per_min\", 5)\n",
    "data.npxg = data.npxg.transform(lambda x: 0.0001 if x == 0 else x)\n",
    "\n",
    "#add features\n",
    "data[\"frac_90\"] = data.minutes/90\n",
    "data[\"ln_frac_90\"] = np.log(data.frac_90)\n",
    "data[\"ln_frac_90_start\"] = data.ln_frac_90 * data.start\n",
    "data[\"ln_frac_90_not_start\"] = data.ln_frac_90 * (1.0 - data.start)\n",
    "data[\"goal_exp_2\"] = data.goal_exp ** 2\n",
    "data[\"supremacy_2\"] = data.supremacy ** 2\n",
    "data[\"is_home\"] = np.where(data.squad == data.home_team, 1, 0)\n",
    "data[\"start\"] = np.where(data.start == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a6915e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_week_difference\n",
    "def get_week_difference(data, current_year, current_week):\n",
    "    def func(row):\n",
    "        d = (current_year - row[\"year\"]) * 52 + (current_week - row[\"week\"])\n",
    "        assert(d >= 0)\n",
    "        return d\n",
    "        \n",
    "    diff = data.apply(func, axis = 1)\n",
    "    return diff\n",
    "\n",
    "#get training weights\n",
    "def get_weights(data, year, week, decay_factor):\n",
    "    if decay_factor != None:\n",
    "        week_diff = get_week_difference(data, year, week)\n",
    "        weights = np.exp(-decay_factor*week_diff)\n",
    "    else:\n",
    "        weights = np.full(len(data), 1.0)\n",
    "        \n",
    "    return weights\n",
    "\n",
    "#standardize\n",
    "def standardize(X, cols_to_standardize, scaler=None):\n",
    "    \n",
    "    if scaler==None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X[cols_to_standardize])\n",
    "    \n",
    "    X[cols_to_standardize] = scaler.transform(X[cols_to_standardize])\n",
    "    return X, scaler\n",
    "\n",
    "#estimate gamma parameters\n",
    "def get_gamma_parameters(y_train, train_preds, test_preds, n_features):\n",
    "            \n",
    "    def estimate_x2_scale(y, mu, n_sample, dof):\n",
    "        resid = np.power(y- mu, 2)\n",
    "        variance = mu ** 2\n",
    "        df_residuals = n_sample - dof\n",
    "        return np.sum(resid / variance) / df_residuals\n",
    "\n",
    "    inv_shape_param =  estimate_x2_scale(y_train, train_preds, len(y_train), n_features)\n",
    "    shape_from_model = 1/inv_shape_param\n",
    "\n",
    "    scales = [m_i /shape_from_model for m_i in test_preds]\n",
    "    shapes = np.full(len(scales), shape_from_model)\n",
    "\n",
    "    return scales, shapes\n",
    "\n",
    "#get log likelihoods given expectancies\n",
    "def calculate_log_likelihood(y_true, parameters, distribution='Poisson',individual_scores=False):\n",
    "    assert(len(y_true) == len(parameters[0]))\n",
    "    if distribution =='Poisson' or distribution == 'Exponential':\n",
    "        assert(len(parameters) == 1)\n",
    "    elif distribution =='Gamma':\n",
    "        assert(len(parameters) == 2)\n",
    "    \n",
    "    if distribution=='Poisson':\n",
    "        expectancies = parameters[0]\n",
    "        probs = poisson.pmf(y_true, expectancies)\n",
    "        ind_log_ll = np.log(probs)\n",
    "        \n",
    "    elif distribution=='Exponential':\n",
    "        expectancies = parameters[0]\n",
    "        probs = expon.pdf(y_true, scale= 1/expectancies)\n",
    "        ind_log_ll = np.maximum(-10000,np.log(probs))\n",
    "        \n",
    "    elif distribution=='Gamma':\n",
    "        scales = parameters[0]\n",
    "        shapes = parameters[1]\n",
    "        probs = gamma.pdf(y_true, a = shapes,scale=scales)\n",
    "        ind_log_ll = np.maximum(-10000,np.log(probs))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Invalid distribution argument passed.')\n",
    "        \n",
    "    \n",
    "    log_ll = np.sum(ind_log_ll)\n",
    "    avg_log_ll = log_ll/len(probs)\n",
    "    \n",
    "    if individual_scores == False:\n",
    "        return log_ll, avg_log_ll\n",
    "    else:\n",
    "        return ind_log_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0572f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consecutive_evaluations(data, decay_factor, model_parameters, features,target_variable,\n",
    "                                cols_to_standardize, cols_to_ohe, distribution, verbose=False):\n",
    "\n",
    "    grouped = data.sort_values([\"year\", \"week\"]).groupby([\"year\", \"week\"], as_index = False)\n",
    "    df_test_preds = []\n",
    "    week_counter = 0\n",
    "    \n",
    "    for i,_ in grouped:\n",
    "        if week_counter > 0:\n",
    "            year = i[0]\n",
    "            week = i[1]\n",
    "            if verbose == True:\n",
    "                print(f\"Year = {year}, week = {week}...\")\n",
    "\n",
    "            #split validation data by week-year\n",
    "            train_data, test_data = split_dataset(data, year, week)\n",
    "            train_weights = get_weights(train_data, year, week, decay_factor)\n",
    "\n",
    "            #get x and y data\n",
    "            X_train, X_test = train_data[features].copy(deep=True), test_data[features].copy(deep=True)\n",
    "            y_train, y_test = train_data[target_variable], test_data[target_variable]\n",
    "            \n",
    "            #standardize\n",
    "            X_train, scaler = standardize(X_train, cols_to_standardize=cols_to_standardize)\n",
    "            X_test, _ = standardize(X_test, cols_to_standardize=cols_to_standardize, scaler=scaler)\n",
    "\n",
    "            #add constant\n",
    "            X_train = sm.add_constant(X_train)\n",
    "            X_test = sm.add_constant(X_test)\n",
    "\n",
    "            #train model\n",
    "            if distribution==\"Gamma\":\n",
    "                model = linear_model.GammaRegressor(fit_intercept=False, alpha = model_parameters[\"regularization_parameter\"], max_iter = 500)\n",
    "                model.fit(X_train, y_train, sample_weight = train_weights)\n",
    "                \n",
    "            elif distribution == \"Poisson\":\n",
    "                model = linear_model.PoissonRegressor(fit_intercept=False, alpha = model_parameters[\"regularization_parameter\"], max_iter = 500)\n",
    "                model.fit(X_train, y_train, sample_weight = train_weights)\n",
    "                \n",
    "            elif distribution == \"Catboost\":\n",
    "                model = CatBoostRegressor(task_type=\"GPU\",\n",
    "                                            devices='0:1',\n",
    "                                            early_stopping_rounds = 50,\n",
    "                                            loss_function=\"Poisson\",\n",
    "                                            **model_parameters)\n",
    "                model.fit(X_train,y=y_train, cat_features = cols_to_ohe,verbose=False, sample_weight = train_weights)\n",
    "                \n",
    "            else:\n",
    "                raise Exception(\"Invalid distribution arg.\")\n",
    "            \n",
    "            #get predictions\n",
    "            train_preds = model.predict(X_train)\n",
    "            test_preds = model.predict(X_test)\n",
    "\n",
    "            #if gamma distribution find shape and scale parameters\n",
    "            if distribution == \"Gamma\":\n",
    "                scales, shapes = get_gamma_parameters(y_train, train_preds, test_preds, X_train.shape[1])\n",
    "                test_data = test_data.assign(pred_exp=test_preds,\n",
    "                                             pred_scale=scales,\n",
    "                                             pred_shape=shapes)\n",
    "\n",
    "                df_test_preds.append(test_data[[\"player_id\", \"home_team\", \"away_team\", \"datetime\",\"pred_exp\", \"pred_scale\", \"pred_shape\"]])\n",
    "\n",
    "            else:\n",
    "                test_data = test_data.assign(pred_exp=test_preds)\n",
    "                df_test_preds.append(test_data[[\"player_id\", \"home_team\", \"away_team\", \"datetime\",\"pred_exp\"]])\n",
    "\n",
    "        week_counter += 1\n",
    "    \n",
    "    df_test_pred = (\n",
    "    pd\n",
    "    .concat(df_test_preds, ignore_index=True)\n",
    "    .merge(\n",
    "        data[[\"player_id\", \"home_team\", \"away_team\", \"datetime\", \"npxg\", \"npg\", \"start\", \"position\"]], \n",
    "        how=\"left\", \n",
    "        on=[\"player_id\", \"home_team\", \"away_team\", \"datetime\"], \n",
    "        validate=\"1:1\"\n",
    "    ))\n",
    "    \n",
    "    #add ll\n",
    "    df_test_pred = df_test_pred.assign(poisson_ll = calculate_log_likelihood(df_test_pred.npg,\n",
    "                                          [df_test_pred.pred_exp],\n",
    "                                          distribution='Poisson',\n",
    "                                          individual_scores=True),\n",
    "                                       \n",
    "                                       exp_ll = calculate_log_likelihood(df_test_pred.npxg,\n",
    "                                          [df_test_pred.pred_exp],\n",
    "                                          distribution='Exponential',\n",
    "                                          individual_scores=True) \n",
    "                                      )\n",
    "    \n",
    "    if distribution == \"Gamma\":\n",
    "        df_test_pred = df_test_pred.assign(gamma_ll = calculate_log_likelihood(df_test_pred.npxg,\n",
    "                                          [df_test_pred.pred_scale, df_test_pred.pred_shape],\n",
    "                                          distribution='Gamma',\n",
    "                                          individual_scores=True))\n",
    "        \n",
    "    return df_test_pred, model, X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d053769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(data, categorical_columns) -> pd.DataFrame:\n",
    "    data_ohe = data.copy(deep=True)\n",
    "    return (\n",
    "        data_ohe\n",
    "        .join(pd.get_dummies(data[categorical_columns].astype(str), dtype=int))\n",
    "    )\n",
    "\n",
    "def get_features(data, cols_to_ohe):\n",
    "    features = [\n",
    "        'goal_exp', 'supremacy', 'frac_90'\n",
    "    ]\n",
    "    \n",
    "    for col in cols_to_ohe:\n",
    "        new_features = [f\"{col}_{val}\" for val in data[col].unique()]\n",
    "        features = features + new_features\n",
    "    \n",
    "    return features\n",
    "\n",
    "target_variable = 'npxg'\n",
    "target_variable_cat = 'npg'\n",
    "\n",
    "cols_to_standardize = [\n",
    "    'goal_exp',\n",
    "    'supremacy',\n",
    "    'ln_frac_90_start',\n",
    "    'ln_frac_90_not_start',\n",
    "    'avg_npxg_per_min_l5',\n",
    "    'avg_npxg_per_min_l10',\n",
    "    'avg_team_conceded_npxg_l5',\n",
    "    'avg_npxg_per_min_over_squad_opp_avg_l5',\n",
    "    'avg_npxg_per_min_over_squad_opp_avg_l10',\n",
    "    'avg_touches_att_3rd_per_min_l5',\n",
    "    'avg_touches_att_pen_area_per_min_l5',\n",
    "]\n",
    "\n",
    "cols_to_ohe = [\n",
    "    'position',\n",
    "    'squad_opp',\n",
    "    'player',\n",
    "    'gk_opp'\n",
    "]\n",
    "\n",
    "cols_other = [\n",
    "    'start',\n",
    "    'is_home'\n",
    "]\n",
    "\n",
    "#data_model = ohe(data, cols_to_ohe)\n",
    "data_catboost = data.copy()\n",
    "\n",
    "#model_features = get_features(data_model, cols_to_ohe)\n",
    "catboost_features = cols_other + cols_to_standardize + cols_to_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "051b7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_grid = [0.001,0.01,0.1,1.0]\n",
    "decay_grid = [0.001,0.01,0.1]\n",
    "\n",
    "if TUNE==True:\n",
    "    grid_scores = []\n",
    "    for reg in regularization_grid:\n",
    "        for dec in decay_grid:\n",
    "            print(f\"Trying reg={reg} and decay factor = {dec}\")\n",
    "            model_parameters = {'regularization_parameter':reg}\n",
    "            df_test_pred, model, _ = get_consecutive_evaluations(data_model, dec, model_parameters,model_features,\n",
    "                                                                 target_variable, cols_to_standardize,\n",
    "                                                                 cols_to_ohe, distribution=\"Gamma\")\n",
    "\n",
    "            ll_exponential = df_test_pred.exp_ll.mean()\n",
    "            ll_poisson =  df_test_pred.poisson_ll.mean()\n",
    "            ll_gamma = df_test_pred.gamma_ll.mean()\n",
    "            r2score  = r2_score(df_test_pred.npg,df_test_pred.pred_exp)\n",
    "\n",
    "            grid_scores.append((reg, dec, r2score, ll_exponential,ll_poisson, ll_gamma))\n",
    "\n",
    "    grid_scores = pd.DataFrame(grid_scores, columns=[\"reg_parameter\",\"decay_parameter\",\"r2score\",\n",
    "                                                         \"ll_exponential\",\"ll_poisson\", \"ll_gamma\"])\n",
    "    \n",
    "    grid_scores.to_pickle(\"df_tuning_results.pickle\")\n",
    "    \n",
    "else:\n",
    "    grid_scores = pd.read_pickle(\"df_tuning_results.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2de8c098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 50,\n",
       " 'depth': 6,\n",
       " 'border_count': 50,\n",
       " 'learning_rate': 0.001,\n",
       " 'l2_leaf_reg': 0.001}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fae84211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>depth</th>\n",
       "      <th>border_count</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterations  depth  border_count  learning_rate  l2_leaf_reg\n",
       "0          50      6            50          0.001        0.001"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hyperparameter_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e60a7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying params={'iterations': 50, 'depth': 6, 'border_count': 50, 'learning_rate': 0.1, 'l2_leaf_reg': 0.01} and decay factor = 0.001\n",
      "Year = 2017, week = 32...\n",
      "Year = 2017, week = 33...\n",
      "Year = 2017, week = 34...\n",
      "Year = 2017, week = 36...\n",
      "Year = 2017, week = 37...\n",
      "Year = 2017, week = 38...\n",
      "Year = 2017, week = 39...\n",
      "Year = 2017, week = 41...\n",
      "Year = 2017, week = 42...\n",
      "Year = 2017, week = 43...\n",
      "Year = 2017, week = 44...\n",
      "Year = 2017, week = 46...\n",
      "Year = 2017, week = 47...\n",
      "Year = 2017, week = 48...\n",
      "Year = 2017, week = 49...\n",
      "Year = 2017, week = 50...\n",
      "Year = 2017, week = 51...\n",
      "Year = 2017, week = 52...\n",
      "Year = 2018, week = 1...\n",
      "Year = 2018, week = 2...\n",
      "Year = 2018, week = 3...\n",
      "Year = 2018, week = 4...\n",
      "Year = 2018, week = 5...\n",
      "Year = 2018, week = 6...\n",
      "Year = 2018, week = 7...\n",
      "Year = 2018, week = 8...\n",
      "Year = 2018, week = 9...\n",
      "Year = 2018, week = 10...\n",
      "Year = 2018, week = 11...\n",
      "Year = 2018, week = 13...\n",
      "Year = 2018, week = 14...\n",
      "Year = 2018, week = 15...\n",
      "Year = 2018, week = 16...\n",
      "Year = 2018, week = 17...\n",
      "Year = 2018, week = 18...\n",
      "Year = 2018, week = 19...\n",
      "Year = 2018, week = 20...\n",
      "Trying params={'iterations': 50, 'depth': 6, 'border_count': 50, 'learning_rate': 0.1, 'l2_leaf_reg': 0.01} and decay factor = 0.01\n",
      "Year = 2017, week = 32...\n",
      "Year = 2017, week = 33...\n",
      "Year = 2017, week = 34...\n",
      "Year = 2017, week = 36...\n",
      "Year = 2017, week = 37...\n",
      "Year = 2017, week = 38...\n",
      "Year = 2017, week = 39...\n",
      "Year = 2017, week = 41...\n",
      "Year = 2017, week = 42...\n",
      "Year = 2017, week = 43...\n",
      "Year = 2017, week = 44...\n",
      "Year = 2017, week = 46...\n",
      "Year = 2017, week = 47...\n",
      "Year = 2017, week = 48...\n",
      "Year = 2017, week = 49...\n",
      "Year = 2017, week = 50...\n",
      "Year = 2017, week = 51...\n",
      "Year = 2017, week = 52...\n",
      "Year = 2018, week = 1...\n",
      "Year = 2018, week = 2...\n",
      "Year = 2018, week = 3...\n",
      "Year = 2018, week = 4...\n",
      "Year = 2018, week = 5...\n",
      "Year = 2018, week = 6...\n",
      "Year = 2018, week = 7...\n",
      "Year = 2018, week = 8...\n",
      "Year = 2018, week = 9...\n",
      "Year = 2018, week = 10...\n",
      "Year = 2018, week = 11...\n",
      "Year = 2018, week = 13...\n",
      "Year = 2018, week = 14...\n",
      "Year = 2018, week = 15...\n",
      "Year = 2018, week = 16...\n",
      "Year = 2018, week = 17...\n",
      "Year = 2018, week = 18...\n",
      "Year = 2018, week = 19...\n",
      "Year = 2018, week = 20...\n",
      "Trying params={'iterations': 50, 'depth': 6, 'border_count': 50, 'learning_rate': 0.1, 'l2_leaf_reg': 0.01} and decay factor = 0.1\n",
      "Year = 2017, week = 32...\n",
      "Year = 2017, week = 33...\n",
      "Year = 2017, week = 34...\n",
      "Year = 2017, week = 36...\n",
      "Year = 2017, week = 37...\n",
      "Year = 2017, week = 38...\n",
      "Year = 2017, week = 39...\n",
      "Year = 2017, week = 41...\n",
      "Year = 2017, week = 42...\n",
      "Year = 2017, week = 43...\n",
      "Year = 2017, week = 44...\n",
      "Year = 2017, week = 46...\n",
      "Year = 2017, week = 47...\n",
      "Year = 2017, week = 48...\n",
      "Year = 2017, week = 49...\n",
      "Year = 2017, week = 50...\n",
      "Year = 2017, week = 51...\n",
      "Year = 2017, week = 52...\n",
      "Year = 2018, week = 1...\n",
      "Year = 2018, week = 2...\n",
      "Year = 2018, week = 3...\n",
      "Year = 2018, week = 4...\n",
      "Year = 2018, week = 5...\n",
      "Year = 2018, week = 6...\n",
      "Year = 2018, week = 7...\n",
      "Year = 2018, week = 8...\n",
      "Year = 2018, week = 9...\n",
      "Year = 2018, week = 10...\n",
      "Year = 2018, week = 11...\n",
      "Year = 2018, week = 13...\n",
      "Year = 2018, week = 14...\n",
      "Year = 2018, week = 15...\n",
      "Year = 2018, week = 16...\n",
      "Year = 2018, week = 17...\n",
      "Year = 2018, week = 18...\n",
      "Year = 2018, week = 19...\n",
      "Year = 2018, week = 20...\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_grid = {\n",
    "    'iterations': [50, 100, 200,400],\n",
    "    'depth': [6,10,12,16],\n",
    "    'border_count': [50],#,100,200],\n",
    "    'learning_rate': [0.1],\n",
    "    'l2_leaf_reg': [0.01]\n",
    "}\n",
    "decay_grid = [0.001,0.01,0.1]\n",
    "train_data_catboost = data_catboost[(data.season == '2017-2018')]\n",
    "\n",
    "def make_grid(param_grid):  \n",
    "    keys=param_grid.keys()\n",
    "    combinations=itertools.product(*param_grid.values())\n",
    "    ds=[dict(zip(keys,cc)) for cc in combinations]\n",
    "    return ds\n",
    "\n",
    "if TUNE==True:\n",
    "    hyperparameter_combs = make_grid(hyperparameter_grid)\n",
    "    grid_params = []\n",
    "    grid_scores = []\n",
    "    \n",
    "    for model_parameters in hyperparameter_combs:\n",
    "        for dec in decay_grid:\n",
    "            print(f\"Trying params={model_parameters} and decay factor = {dec}\")\n",
    "            \n",
    "            df_test_pred, model, _ = get_consecutive_evaluations(train_data_catboost, dec, model_parameters,\n",
    "                                                                 catboost_features,target_variable, \n",
    "                                                                 cols_to_standardize,cols_to_ohe,\n",
    "                                                                 distribution=\"Catboost\",\n",
    "                                                                 verbose=True)\n",
    "\n",
    "            ll_poisson =  df_test_pred.poisson_ll.mean()\n",
    "            r2score  = r2_score(y_true=df_test_pred.npg,y_pred=df_test_pred.pred_exp)\n",
    "\n",
    "            grid_scores.append((dec, r2score, ll_exponential,ll_poisson))\n",
    "            grid_params.append(model_parameters)\n",
    "\n",
    "    grid_scores = pd.DataFrame(grid_scores, columns=[\"decay_parameter\",\"r2score\",\"ll_exponential\",\"ll_poisson\"])\n",
    "    grid_params = pd.DataFrame(grid_params)\n",
    "    \n",
    "    grid_scores = pd.concat([grid_params, grid_scores],axis=1).sort_values()\n",
    "    grid_scores.to_pickle(\"df_tuning_results.pickle\")\n",
    "    \n",
    "else:\n",
    "    grid_scores = pd.read_pickle(\"df_tuning_results.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "660ce154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>depth</th>\n",
       "      <th>border_count</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>decay_parameter</th>\n",
       "      <th>r2score</th>\n",
       "      <th>ll_exponential</th>\n",
       "      <th>ll_poisson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.118115</td>\n",
       "      <td>-0.134466</td>\n",
       "      <td>-0.276929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.118605</td>\n",
       "      <td>-0.134466</td>\n",
       "      <td>-0.276825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.117408</td>\n",
       "      <td>-0.134466</td>\n",
       "      <td>-0.277203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterations  depth  border_count  learning_rate  l2_leaf_reg  \\\n",
       "0          50      6            50            0.1         0.01   \n",
       "1          50      6            50            0.1         0.01   \n",
       "2          50      6            50            0.1         0.01   \n",
       "\n",
       "   decay_parameter   r2score  ll_exponential  ll_poisson  \n",
       "0            0.001  0.118115       -0.134466   -0.276929  \n",
       "1            0.010  0.118605       -0.134466   -0.276825  \n",
       "2            0.100  0.117408       -0.134466   -0.277203  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8af373",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_param = 0.01\n",
    "reg_param = 0.001\n",
    "\n",
    "df_test_pred, model, _ = get_consecutive_evaluations(data_model, decay_param, reg_param,model_features,target_variable,\n",
    "                                                        cols_to_standardize,distribution=\"Gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99265111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip([\"const\"] + model_features, model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54035d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = expectancies.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_train = pd.merge(model_results,\n",
    "                         train_data[[\"datetime\",\"date\",\"squad\",\"position_mapped\", \"start\", \"player\",\"npxg\",\"npg\"]],\n",
    "                         on = [\"date\",\"squad\",\"player\"],\n",
    "                         how='inner')\n",
    "\n",
    "\n",
    "model_results_validation = pd.merge(model_results,\n",
    "                         val_data[[\"datetime\",\"date\",\"squad\",\"position_mapped\", \"start\", \"player\",\"npxg\",\"npg\"]],\n",
    "                         on = [\"date\",\"squad\",\"player\"],\n",
    "                         how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a038fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get exponential fit ll\n",
    "ind_llres = get_log_likelihood(model_results_validation.npxg,\n",
    "                               (model_results_validation.scale, model_results_validation[\"shape\"]), \n",
    "                               distribution='Gamma',\n",
    "                               individual_scores=True)\n",
    "\n",
    "llres = get_log_likelihood(model_results_validation.npxg,\n",
    "                           (model_results_validation.scale, model_results_validation[\"shape\"]), \n",
    "                           distribution='Gamma',\n",
    "                           individual_scores=False)\n",
    "\n",
    "r2score = r2_score(model_results_validation.npxg, model_results_validation.expectancies)\n",
    "print(llres, r2score)\n",
    "model_results_validation[\"ll\"] = ind_llres\n",
    "\n",
    "#get poisson fit ll \n",
    "ind_llres = get_log_likelihood(model_results_validation.npg,\n",
    "                               (model_results_validation.expectancies,), \n",
    "                               distribution='Poisson',\n",
    "                               individual_scores=True)\n",
    "\n",
    "llres = get_log_likelihood(model_results_validation.npg,\n",
    "                           (model_results_validation.expectancies,), \n",
    "                           distribution='Poisson',\n",
    "                           individual_scores=False)\n",
    "\n",
    "print(llres)\n",
    "model_results_validation[\"ll_poisson\"] = ind_llres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_mapping_function(x_train,x_val, encoding_dict):    \n",
    "    \n",
    "    #one hot encode x_train and x_test\n",
    "    x_train, encoder = one_hot_encode_data(x_train, encoding_dict=encoding_dict)\n",
    "    x_val, _ = one_hot_encode_data(x_val, encoding_dict=encoding_dict, preprocessor=encoder)\n",
    "    \n",
    "    #multiply npxg by OHE variables and drop npxg col\n",
    "    x_train = x_train.multiply(x_train[\"expectancies_log\"], axis=\"index\")\n",
    "    x_train.expectancies_log = np.sqrt(x_train.expectancies_log)\n",
    "    \n",
    "    x_val = x_val.multiply(x_val[\"expectancies_log\"], axis=\"index\")\n",
    "    x_val.expectancies_log = np.sqrt(x_val.expectancies_log)\n",
    "    \n",
    "    return x_train, x_val    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be476f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5339dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_pmf(df, sample_size, pmf_support):   \n",
    "    # Initialize array to store average PMFs\n",
    "    avg_pmfs = np.zeros((len(df), 10))\n",
    "    \n",
    "    # Generate gamma samples in bulk for all rows\n",
    "    gamma_samples = gamma.rvs(a=df['shape'].values[:, np.newaxis], scale=df['scale'].values[:, np.newaxis], size=(len(df), sample_size))\n",
    "    \n",
    "    # Compute Poisson PMFs for all gamma samples (vectorized operation)\n",
    "    sample_pmfs = poisson.pmf(pmf_support[np.newaxis, np.newaxis, :], gamma_samples[:, :, np.newaxis])\n",
    "    \n",
    "    # Compute the average PMF across all samples (axis=1)\n",
    "    avg_pmfs = np.mean(sample_pmfs, axis=1)\n",
    "    \n",
    "    return avg_pmfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff514f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare PMF using single point estimate for xg (expectancies)\n",
    "x = np.arange(0, 6)\n",
    "probs= poisson.pmf(x, model_results_validation.expectancies.values[:,np.newaxis])\n",
    "probs_mean = np.mean(probs, axis=0)\n",
    "true_dist = model_results_validation.npg.value_counts(normalize=True).sort_index()\n",
    "\n",
    "plt.plot(x,probs_mean, label=\"Pred ngp\")\n",
    "true_dist.plot(label=\"True npg\")\n",
    "plt.legend()\n",
    "\n",
    "[print(i) for i in enumerate(list(zip(probs_mean, true_dist.values)))]\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#compare PMF using sampled estimate\n",
    "sampled_distributions = compute_avg_pmf(model_results_validation, sample_size=4000, pmf_support=np.arange(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_comparison = model_results_validation[(model_results_validation.datetime >= '2020-09-19 16:30:00') &\\\n",
    "                                          (model_results_validation.start == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9072ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_comparison[\"ll_poisson\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'FW'\n",
    "df_position = ben_comparison[ben_comparison.position_mapped == position]\n",
    "df_position.ll_poisson.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca02005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_train.to_csv(\"C:/MyDevelopment/Goalscorers/model_results/npxg_train_predictions.csv\", index=False)\n",
    "model_results_validation.to_csv(\"C:/MyDevelopment/Goalscorers/model_results/npxg_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078d447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
